I"<h1 id="call-for-papers">Call for Papers</h1>

<p>The 1st Workshop on <a href="http://www.learn-to-race-challenge.com.s3-website-us-east-1.amazonaws.com/">Safe Learning for Autonomous Driving</a> is co-located with the International Conference on Learning Representations, to be held from April 25-29, 2022 (Virtual).</p>

<p>Existing research on autonomous driving primarily focuses on urban driving, which is insufficient for characterising the complex driving behaviour underlying high-speed racing. At the same time, existing racing simulation frameworks struggle in capturing realism, with respect to visual rendering, vehicular dynamics, and task objectives, inhibiting the transfer of learning agents to real-world contexts. The Safe Learning for Autonomous Driving workshop provides a venue for research and standardized experimentation on high-speed autonomous racing. Participants can choose to take part in the Learn to Race competition or directly submit papers to the workshop via the call for papers.</p>

<h2 id="workshop-publication-topics">Workshop Publication Topics</h2>

<p>We are accepting papers in the following broad areas of safe vehicle autonomy, including (but not limited to) the following:</p>

<ul>
  <li>Leveraging Offline Demonstrations in Online Learning</li>
  <li>Constrained Reinforcement Learning</li>
  <li>Physics-guided Reinforcement Learning</li>
  <li>Fast Online Optimisation and Decision-making</li>
  <li>Transfer Learning and Domain Adaptation</li>
  <li>Architectures for Distributed Optimisation</li>
  <li>Representation Learning and Multimodal Alignment</li>
  <li>Virtual Sensing and Sensor Translation</li>
  <li>Dependability Analysis for Learning-based Systems</li>
</ul>

<h2 id="workshop-challenge-topics">Workshop Challenge Topics</h2>

<p>Through this workshop, we are launching an AI challenge with two subtasks:</p>

<ul>
  <li>Sub-task #1: Learn-to-Race (L2R): Maximise performance in simulated Formula-style autonomous racing, using the L2R environment.</li>
  <li>Sub-task #2: Safety-aware learning: Maximise both agent performance and safety in the L2R environment.</li>
</ul>

<h2 id="submission-guidelines">Submission Guidelines</h2>

<p>We are accepting papers for three tracks</p>
<ul>
  <li>Full papers (9 pages, excluding references) – to be included in the ICLR 2022 Workshop Proceedings</li>
  <li>Challenge submissions (4 pages, excluding references) – to be included in the ICLR 2022 Workshop Proceedings</li>
  <li>Short papers (4 pages, excluding references)</li>
</ul>

<p>Submissions are handled through <a href="https://openreview.net/group?id=ICLR.cc/2022/">OpenReview</a>. We will follow the submission guidelines specified by ICLR 2022 which can be found <a href="https://iclr.cc/Conferences/2022/CallForPapers">here</a></p>

<h2 id="organising-committee">Organising Committee</h2>

<ul>
  <li>Jonathan Francis; CMU, Bosch Research</li>
  <li>Siddha Ganju; NVIDIA</li>
  <li>James Herman; CMU</li>
  <li>Bingqing Chen; CMU</li>
  <li>Sylvia L. Herbert; UCSD</li>
  <li>Jaime Fisac; Princeton</li>
  <li>Jean Oh; CMU</li>
  <li>Eric Nyberg; CMU</li>
  <li>Rowan McAllister; Berkeley, Toyota Research Institute</li>
  <li>Felipe Codevilla; MILA</li>
</ul>

<h2 id="publication">Publication</h2>

<p>Papers from both the main track and the challenge tracks will be included in the workshop proceedings of ICLR 2022.</p>

<h2 id="contact-addresses">Contact addresses</h2>

<ul>
  <li>Re: general inquiries: sl4ad.workshop+info [AT] gmail.com</li>
  <li>Re: submissions of papers: sl4ad.workshop+papers [AT] gmail.com</li>
  <li>Re: challenge: sl4ad.workshop+challenge [AT] gmail.com</li>
</ul>

<h2 id="important-dates-all-tracks">Important Dates (all tracks)</h2>

<ul>
  <li>Abstract Submission: 19 February 2022</li>
  <li>Paper submission: 26 February 2022</li>
  <li>Notification of acceptance: Mar 26, 2022</li>
  <li>Challenge entry submission deadline (to be featured at the Workshop): April 1, 2022</li>
  <li>Camera-ready paper submission: April 8, 2022</li>
  <li>Workshop Date: April 25, 2022</li>
</ul>
:ET